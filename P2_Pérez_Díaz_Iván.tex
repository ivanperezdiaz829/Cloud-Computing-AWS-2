\documentclass[a4paper,12pt]{article}

% -------------------------------------------------
% PAQUETES BÁSICOS
% -------------------------------------------------
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Fuentes
\usepackage{mathptmx} % Times New Roman
\usepackage{helvet}   % Arial (usada en títulos)
\renewcommand{\familydefault}{\rmdefault}

% Márgenes
\usepackage[
  top=2.5cm,
  bottom=2.5cm,
  right=2.5cm,
  left=3cm
]{geometry}

% Interlineado
\usepackage{setspace}
\onehalfspacing

% Justificación
\usepackage{ragged2e}
\justifying

% Gráficos y tablas
\usepackage{graphicx}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{float}

% Código fuente
\usepackage{listings}
\usepackage{xcolor}
\lstdefinelanguage{PowerShell}{
  morekeywords={aws, kinesis, create-stream},
  sensitive=false,
  morecomment=[l]{\#},
  morestring=[b]",
}
\lstdefinelanguage{Python}{
  keywords={def, return, if, else, elif, for, while, import, from, as, class},
  sensitive=true,
  morecomment=[l]{\#},
  morestring=[b]",
}

% Hipervínculos
\usepackage{hyperref}

% Encabezado y pie
\usepackage{fancyhdr}
\usepackage{lastpage}

% -------------------------------------------------
% ENCABEZADO Y PIE DE PÁGINA
% -------------------------------------------------
\pagestyle{fancy}
\fancyhf{}

\fancyhead[R]{Computación en la Nube – Práctica 7}
\fancyfoot[C]{Página \thepage\ de \pageref{LastPage}}

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% -------------------------------------------------
% FORMATO DE TÍTULOS
% -------------------------------------------------
\usepackage{titlesec}

\titleformat{\section}
  {\normalfont\sffamily\bfseries\fontsize{14}{16}\selectfont}
  {\thesection}{1em}{}

\titleformat{\subsection}
  {\normalfont\sffamily\bfseries\fontsize{13}{15}\selectfont}
  {\thesubsection}{1em}{}

\titleformat{\subsubsection}
  {\normalfont\sffamily\bfseries\fontsize{12}{14}\selectfont}
  {\thesubsubsection}{1em}{}

% -------------------------------------------------
% CONFIGURACIÓN DE CÓDIGO
% -------------------------------------------------
\lstset{
  basicstyle=\ttfamily\fontsize{10}{12}\selectfont,
  backgroundcolor=\color[HTML]{F5F5F5},
  frame=single,
  breaklines=true,
  captionpos=b,
  numbers=left,
  numberstyle=\tiny,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{teal},
  tabsize=2
}

%------------------------------------------------------------------------------------
\begin{document}

\begin{titlepage}
  \centering

  \vspace*{\fill} % Empuja el contenido hacia el centro vertical

  {\Huge \textbf{Práctica 7}}\\[0.5cm]
  {\Large Computación en la Nube}\\[2cm]

  {\Large \textbf{Universidad de Las Palmas de Gran Canaria}}\\[0.3cm]
  {\large Escuela de Ingeniería Informática}\\[0.3cm]
  {\large Grado en Ingeniería Informática}\\[1.5cm]

  \textbf{Curso académico:} 2025--2026\\
  \textbf{Estudiante:} Iván Pérez Díaz\\
  \textbf{Fecha de entrega:} \today

  \vspace*{\fill} % Empuja el contenido hacia el centro vertical
\end{titlepage}
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\tableofcontents
\newpage
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\section{Introducción}

El objetivo principal de esta práctica es el diseño, despliegue y validación de una arquitectura 
completa utilizando el ecosistema de servicios gestionados de Amazon Web Services (AWS). Se busca implementar un pipeline de datos 
robusto que abarque desde la generación de la información hasta su transformación final para 
la toma de decisiones.

La solución adopta un enfoque \textit{serverless} (sin servidor), minimizando la carga operativa 
de administración de infraestructura. Los componentes clave seleccionados son:
\begin{itemize}
    \item \textbf{Amazon S3:} Actúa como la capa de persistencia centralizada (Data Lake), 
    proporcionando almacenamiento escalable y duradero.
    \item \textbf{Amazon Kinesis Data Streams:} Facilita la ingesta de datos de alta velocidad 
    y baja latencia, desacoplando los productores de los consumidores.
    \item \textbf{Amazon Kinesis Data Firehose:} Gestiona la entrega fiable de los datos en streaming 
    hacia el almacenamiento, gestionando buffers y transformaciones intermedias.
    \item \textbf{AWS Glue:} Proporciona un entorno unificado para el descubrimiento de metadatos (Data Catalog) 
    y la ejecución de trabajos ETL (Extract, Transform, Load).
\end{itemize}

La arquitectura implementada procesa un conjunto de datos referente a un inventario de 
dispositivos portátiles. Cada registro contiene atributos heterogéneos como marca, modelo, 
resolución de pantalla, especificaciones de hardware (CPU, RAM, GPU), sistema operativo y precio (\texttt{deploy.ps1}). 
El flujo termina con la generación de datasets analíticos en formato columnar, listos 
para ser consultados mediante SQL.
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\section{Desarrollo de las actividades}
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\subsection{Configuración del bucket S3}

La creación y 
configuración del bucket S3 se realiza de forma automatizada mediante el script \texttt{deploy.ps1}, 
aplicando principios de Infraestructura como Código (IaC). El nombre del bucket se genera de forma 
dinámica concatenando el prefijo \texttt{datalake-laptops-} con el identificador único de la cuenta AWS, 
asegurando así un espacio de nombres globalmente único.

La estructura de carpetas definida sigue las mejores prácticas de organización de datos por capas ("zonas"), 
lo que facilita la gobernanza y el ciclo de vida de la información:

\begin{itemize}
  \item \texttt{raw/}: Conocida como "Landing Zone". Almacena los datos en bruto tal cual son 
  recibidos desde Kinesis Firehose, en formato JSON. Esta capa es inmutable y sirve como fuente 
  de verdad histórica.
  \item \texttt{processed/}: O "Curated Zone". Contiene los resultados refinados generados por 
  los trabajos ETL. Los datos aquí están limpios, enriquecidos y convertidos a formato Parquet 
  para optimizar el rendimiento de lectura y reducir costes de almacenamiento.
  \item \texttt{config/}: Repositorio para ficheros de configuración externos o parámetros que 
  puedan requerir los procesos.
  \item \texttt{scripts/}: Almacenamiento de los códigos fuente Python/Spark utilizados por los 
  trabajos de AWS Glue.
  \item \texttt{logs/}: Centralización de registros de eventos y errores para facilitar la depuración 
  y auditoría del sistema.
\end{itemize}

Esta segmentación permite aplicar políticas de seguridad y retención diferenciadas según la criticidad 
y el uso de los datos en cada etapa.
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\subsection{Implementación del productor de datos}

El componente productor es una aplicación desarrollada en Python que simula la generación de eventos 
de negocio en tiempo real. Utilizando la librería \texttt{boto3} (SDK de AWS para Python), el script 
lee un dataset base (\texttt{datos.json}) y envía registros individuales al servicio Amazon Kinesis 
Data Streams.

Para garantizar una simulación realista de un entorno de streaming, el productor introduce latencias 
artificiales entre envíos. Cada registro enviado incluye una clave de partición (\textit{Partition Key}), 
en este caso basada en la marca del dispositivo (\texttt{Company}), lo que asegura que los datos 
relacionados se dirijan al mismo shard, manteniendo el orden secuencial si fuera necesario.

\begin{lstlisting}[language=PowerShell, caption={Comando de creación del Stream con capacidad provisionada}]
aws kinesis create-stream --stream-name energy-stream --shard-count 1
\end{lstlisting}

El uso de \texttt{shard-count 1} define la capacidad inicial de ingesta (1 MB/segundo o 1000 registros/segundo), 
suficiente para el volumen de datos de esta práctica, aunque escalable horizontalmente según la demanda. 
El código completo del productor, incluyendo el manejo de excepciones y logging, se incluye en el Anexo.
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\subsection{Configuración del consumidor (Kinesis Firehose)}

Amazon Kinesis Data Firehose actúa como el puente entre el flujo de datos en tiempo real y el 
almacenamiento persistente. Se ha configurado un \textit{Delivery Stream} que consume datos directamente 
del stream de Kinesis y los deposita en la ruta \texttt{raw/} del bucket S3.

Una característica crítica implementada en esta fase es la \textbf{Transformación de Datos mediante Lambda}. 
Antes de escribir en S3, Firehose invoca una función AWS Lambda (\texttt{laptops-firehose-lambda}) que realiza 
dos tareas fundamentales:
\begin{enumerate}
    \item \textbf{Enriquecimiento temporal:} Añade un campo \texttt{processed\_at} con la marca de tiempo UTC exacta del procesamiento.
    \item \textbf{Formateo y Particionamiento:} Convierte el JSON estándar en JSON Lines (un registro por línea) y define las claves de partición dinámicas (\texttt{partitionKeyFromLambda}).
\end{enumerate}

Esto habilita el "Particionamiento Dinámico" en S3, organizando los archivos físicamente en carpetas por fecha 
(e.g., \texttt{processing\_date=2023-10-27/}), lo que mejora drásticamente la eficiencia de los crawlers y las consultas futuras.

\begin{lstlisting}[language=Python, caption={Lógica de particionado y time-stamping en Lambda}]
# Agrega timestamp de procesamiento
data_json['processed_at'] = processing_time.isoformat()
# Define la estructura de carpetas en S3 basada en la fecha
partition_date = processing_time.strftime('%Y-%m-%d')
\end{lstlisting}

Además, se ha configurado un buffer en Firehose (por tamaño o por tiempo, 64MB o 60 segundos) para optimizar el tamaño de los archivos resultantes y reducir las llamadas a la API de S3.
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\subsection{Configuración de AWS Glue}

AWS Glue se utiliza como componente central de integración de datos. La configuración se divide en dos fases: 
metadatos y procesamiento.

Primero, se define el \textbf{Data Catalog}. Se crea la base de datos lógica \texttt{laptops\_db} y se despliega un 
Crawler (\texttt{laptops-raw-crawler}). Este crawler inspecciona periódicamente la ruta \texttt{raw/} en S3, infiere el esquema 
de los archivos JSON (detectando tipos de datos como string, double, integer) y actualiza la tabla \texttt{laptops\_json} en el catálogo. 
Esto permite que los datos no estructurados sean consultables como si fueran una tabla relacional.

Posteriormente, se implementan dos trabajos (\textit{Glue Jobs}) basados en Spark:
\begin{itemize}
    \item \textbf{Analytics por Marca:} Agrega datos por fabricante, calculando métricas como precio medio, máximo y conteo de modelos.
    \item \textbf{Analytics por SO:} Realiza un análisis similar segmentado por sistema operativo.
\end{itemize}

Un desafío técnico resuelto en los scripts fue la inconsistencia de tipos de datos (Schema Evolution). Se utilizó la función 
\texttt{resolveChoice} de DynamicFrame para castear explícitamente el campo \texttt{Price\_euros} a \texttt{double}, evitando errores 
cuando Spark encontraba enteros y decimales mezclados. Finalmente, los datos se escriben en formato \textbf{Parquet} con compresión 
\textbf{Snappy}, un estándar de la industria que ofrece un esquema columnar altamente eficiente para analítica.
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\section{Diagrama del flujo de datos}

La arquitectura implementada establece un pipeline unidireccional que garantiza la integridad y trazabilidad del dato. 
El flujo de la información atraviesa las siguientes etapas secuenciales:

\begin{enumerate}
    \item \textbf{Generación:} El script productor inyecta registros JSON en el Stream de Kinesis.
    \item \textbf{Buffer y Transformación:} Kinesis Firehose recoge los registros, invoca a la función Lambda para añadir metadatos y acumula los datos en memoria.
    \item \textbf{Almacenamiento Raw:} Firehose vuelca los bloques de datos en la zona \texttt{raw} de S3, particionados por fecha.
    \item \textbf{Descubrimiento:} El Crawler de Glue escanea S3 y actualiza el esquema en el Catálogo de Datos.
    \item \textbf{Procesamiento Batch:} Los Glue Jobs leen del catálogo, ejecutan transformaciones Spark en memoria distribuida y agregan la información.
    \item \textbf{Almacenamiento Processed:} Los resultados finales se escriben en la zona \texttt{processed} de S3 en formato Parquet.
\end{enumerate}

La Figura~\ref{fig:flujo-datos} ilustra gráficamente esta topología, destacando la separación entre la capa de streaming (Kinesis) y la capa batch (Glue).
% Aquí se incluye el diagrama de arquitectura.
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\section{Presupuesto y estimación de costes}

La gestión de costes es fundamental en arquitectura Cloud. La estimación presentada a continuación se basa en la calculadora oficial de AWS (AWS Pricing Calculator) para la región \texttt{us-east-1}, considerando un escenario de operación mensual estándar.

El modelo de costes varía según el servicio:
\begin{itemize}
    \item \textbf{Kinesis:} Se factura por "Shard Hour". Un shard activo 24/7 tiene un coste fijo, independientemente del tráfico (hasta su límite).
    \item \textbf{S3:} Modelo "Pay-as-you-go" basado en almacenamiento GB/mes y número de peticiones PUT/GET.
    \item \textbf{Glue:} Facturación por DPU-Hour (Data Processing Unit). Se cobra por el tiempo de ejecución de los jobs y crawlers, con un mínimo de facturación.
    \item \textbf{Lambda:} Coste basado en número de invocaciones y GB-segundo de memoria utilizada.
\end{itemize}

\begin{table}[H]
\centering
\caption{Estimación detallada de costes de servicios AWS (Escenario mensual)}
\begin{tabular}{lcc}
\toprule
Servicio & Coste mensual (€) & Coste anual (€) \\
\midrule
Amazon S3 (Almacenamiento y Peticiones) & 5 & 60 \\
Amazon Kinesis Data Streams (1 Shard) & 15 & 180 \\
AWS Glue (Crawlers y Jobs ETL diarios) & 10 & 120 \\
AWS Lambda (Computación y Free Tier) & 2 & 24 \\
\midrule
\textbf{Total Estimado} & \textbf{32} & \textbf{384} \\
\bottomrule
\end{tabular}
\end{table}

Cabe destacar que, para cargas de trabajo de laboratorio o desarrollo, muchos de estos costes se mitigan gracias a la Capa Gratuita (Free Tier) de AWS, especialmente en Lambda y S3.
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\section{Conclusiones}

La realización de esta práctica ha permitido validar experimentalmente una arquitectura moderna de Data Lake sobre AWS. Se ha demostrado cómo la combinación de servicios desacoplados (Kinesis para transporte, S3 para almacenamiento, Glue para cómputo) ofrece una solución escalable, flexible y mantenible.

Entre las principales lecciones aprendidas y dificultades superadas, destacan:
\begin{itemize}
    \item \textbf{Gestión de Tipos de Datos:} La importancia de manejar la heterogeneidad de los datos (enteros vs floats) en Spark para evitar fallos en tiempo de ejecución.
    \item \textbf{Particionamiento:} Cómo el particionamiento dinámico en la ingesta mejora significativamente la organización del Data Lake.
    \item \textbf{Infraestructura como Código:} El valor de utilizar scripts (PowerShell/AWS CLI) para desplegar recursos de forma repetible frente a la configuración manual en consola.
\end{itemize}

Como líneas de trabajo futuro, la arquitectura podría evolucionar incorporando \textbf{Amazon Athena} para consultas SQL interactivas sobre los datos procesados, o \textbf{Amazon QuickSight} para la visualización de cuadros de mando (dashboards) de inteligencia de negocios.
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\section{Referencias y bibliografía}

\begin{itemize}
  \item Amazon Web Services. (2024). \textit{Amazon Kinesis Data Streams Developer Guide}. Recuperado de la documentación oficial de AWS.
  \item Amazon Web Services. (2024). \textit{AWS Glue Developer Guide}. Sección sobre ETL Jobs y DynamicFrames.
  \item Amazon Web Services. (2024). \textit{Amazon S3 User Guide}. Best practices for Data Lakes.
  \item Apache Spark. (2024). \textit{PySpark Documentation}. DataFrame API y tipos de datos.
\end{itemize}
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\section{Anexos}

En este apartado se incluye el código fuente completo desarrollado para la orquestación, ingesta y transformación de datos. Estos scripts constituyen la base operativa de la práctica.

\subsection{Anexo A. Script de despliegue}
Este script automatiza la creación de la infraestructura (Buckets, Roles, Streams, Lambdas y Jobs).
\lstinputlisting[
  language=PowerShell,
  caption={Script deploy.ps1: Automatización de infraestructura AWS}
]{./deploy.ps1}

\subsection{Anexo B. Consumidor Firehose}
Función Lambda encargada de la transformación y particionado temporal de los datos antes de su persistencia.
\lstinputlisting[
  language=Python,
  caption={Script firehose.py: Lógica de transformación Lambda}
]{./firehose.py}

\subsection{Anexo C. Análisis por marca}
Job de Spark ETL para la agregación de métricas de negocio basadas en el fabricante.
\lstinputlisting[
  language=Python,
  caption={Script laptops\_analytics\_brand.py: ETL de marcas}
]{./laptops_analytics_brand.py}

\subsection{Anexo D. Análisis por sistema operativo}
Job de Spark ETL para la agregación de métricas basadas en el Sistema Operativo.
\lstinputlisting[
  language=Python,
  caption={Script laptops\_analytics\_so.py: ETL de Sistemas Operativos}
]{./laptops_analytics_so.py}

\subsection{Anexo E. Crawler de AWS Glue}
Configuración JSON utilizada para desplegar el Crawler de descubrimiento de metadatos.
\lstinputlisting[
  language=PowerShell,
  caption={Script aws\_crawler.ps1: Definición del Crawler}
]{./aws_crawler.ps1}

\subsection{Anexo F. Script de limpieza}
Script crítico para la eliminación de recursos y prevención de costes tras la finalización del laboratorio.
\lstinputlisting[
  language=PowerShell,
  caption={Script clean.ps1: Destrucción de recursos}
]{./clean.ps1}

Se ha utilizado inteligencia artificial únicamente como herramienta de apoyo
para la redacción y corrección de estilo de la memoria técnica, garantizando que todo el código y la arquitectura han sido implementados y validados por el estudiante.
%------------------------------------------------------------------------------------

\end{document}