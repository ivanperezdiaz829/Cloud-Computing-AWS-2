\documentclass[a4paper,12pt]{article}

% -------------------------------------------------
% 1. CODIFICACIÓN E IDIOMA
% -------------------------------------------------
\usepackage[spanish,es-tabla]{babel} % es-tabla para que diga "Tabla" en vez de "Cuadro"
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% -------------------------------------------------
% 2. FUENTES Y TIPOGRAFÍA
% -------------------------------------------------
% Cuerpo del texto: Times New Roman
\usepackage{mathptmx}

% Títulos: Arial (Helvética es el clon estándar en LaTeX)
\usepackage{helvet}

% Código fuente: Courier (Cumple requisito de monoespaciada específica)
\usepackage{courier}

% Establecer Times como la fuente por defecto
\renewcommand{\familydefault}{\rmdefault}

% -------------------------------------------------
% 3. MÁRGENES Y GEOMETRÍA
% -------------------------------------------------
% Requisito: Sup, Inf, Der: 2.5cm | Izq: 3cm
\usepackage[
  top=2.5cm,
  bottom=2.5cm,
  right=2.5cm,
  left=3cm
]{geometry}

% -------------------------------------------------
% 4. INTERLINEADO Y ALINEACIÓN
% -------------------------------------------------
% Requisito: 1.5 líneas
\usepackage{setspace}
\onehalfspacing

% Requisito: Texto justificado
\usepackage{ragged2e}
\justifying

% -------------------------------------------------
% 5. ELEMENTOS GRÁFICOS (FIGURAS Y TABLAS)
% -------------------------------------------------
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{caption}

% Configuración de pies de foto y títulos
\captionsetup[table]{position=top, skip=5pt}    % Asegura espacio si el título va arriba
\captionsetup[figure]{position=bottom, skip=5pt} % Asegura espacio si el pie va abajo

% -------------------------------------------------
% 6. CÓDIGO FUENTE (LISTINGS)
% -------------------------------------------------
\usepackage{listings}
\usepackage{xcolor}

% Definición de lenguajes extra
\lstdefinelanguage{PowerShell}{
  morekeywords={aws, kinesis, create-stream},
  sensitive=false,
  morecomment=[l]{\#},
  morestring=[b]",
}
\lstdefinelanguage{Python}{
  keywords={def, return, if, else, elif, for, while, import, from, as, class, try, except},
  sensitive=true,
  morecomment=[l]{\#},
  morestring=[b]",
}

% Configuración visual del código
% Requisito: Fuente monoespaciada 10pt, fondo gris claro (#F5F5F5)
\lstset{
  basicstyle=\ttfamily\fontsize{10}{12}\selectfont, % \ttfamily usará Courier gracias al paquete cargado arriba
  backgroundcolor=\color[HTML]{F5F5F5},
  frame=single,             % Recuadro
  rulecolor=\color{gray},   % Color del borde
  breaklines=true,
  captionpos=b,             % Títulos de código abajo (opcional, cambiar a 't' si se prefiere arriba)
  numbers=left,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{teal},
  tabsize=2,
  showstringspaces=false
}

% -------------------------------------------------
% 7. HIPERVÍNCULOS
% -------------------------------------------------
\usepackage[hidelinks]{hyperref}
\hypersetup{
  pdfborderstyle={/S/U/W 1},
  linkbordercolor={0.7 0.7 0.7},
  urlbordercolor={0.7 0.7 0.7},
}

% -------------------------------------------------
% 8. ENCABEZADO Y PIE DE PÁGINA
% -------------------------------------------------
\usepackage{fancyhdr}
\usepackage{lastpage}

\pagestyle{fancy}
\fancyhf{}

% Requisito: Nombre asignatura/práctica alineado a la derecha
\fancyhead[R]{Computación en la Nube – Práctica 7}

% Requisito: "Página X de Y" centrado en el pie
\fancyfoot[C]{Página \thepage\ de \pageref{LastPage}}

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% -------------------------------------------------
% 9. FORMATO DE TÍTULOS (SECCIONES)
% -------------------------------------------------
\usepackage{titlesec}

% Requisito: Arial (sffamily), Negrita.
% H1 (Section): 14 pt
\titleformat{\section}
  {\normalfont\sffamily\bfseries\fontsize{14}{16}\selectfont}
  {\thesection}{1em}{}

% H2 (Subsection): 13 pt
\titleformat{\subsection}
  {\normalfont\sffamily\bfseries\fontsize{13}{15}\selectfont}
  {\thesubsection}{1em}{}

% H3 (Subsubsection): 12 pt
\titleformat{\subsubsection}
  {\normalfont\sffamily\bfseries\fontsize{12}{14}\selectfont}
  {\thesubsubsection}{1em}{}

%------------------------------------------------------------------------------------
\begin{document}

\begin{titlepage}
  \centering

  \vspace*{2cm} % Empuja el contenido hacia el centro vertical

  {\LARGE \textbf{Universidad de Las Palmas de Gran Canaria}}\\[0.3cm]
  {\large Escuela de Ingeniería Informática}\\[0.3cm]
  {\large Grado en Ingeniería Informática}\\[1.5cm]

  \vspace*{2cm}

  {\Huge \textbf{Práctica 7}}\\[0.5cm]
  {\Large Computación en la Nube}\\[2cm]

  \vspace*{2cm}

  \large{\textbf{Curso académico:} 2025--2026}\\
  \large{\textbf{Estudiante:} Iván Pérez Díaz}\\
  \large{\textbf{Fecha de entrega:} \today}

  \vspace*{\fill} % Empuja el contenido hacia el centro vertical
\end{titlepage}
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\tableofcontents
\newpage
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\section{Introducción}

El objetivo principal de esta práctica es el diseño, despliegue y validación de una 
arquitectura completa en la nube, utilizando el ecosistema de servicios gestionados de 
Amazon Web Services (AWS). Se busca implementar un pipeline de datos robusto que abarque 
desde la generación de la información en tiempo real hasta las validaciones finales con SQL.

La solución adopta un enfoque \textit{serverless} (sin servidor), minimizando la carga 
operativa de administración de infraestructura y permitiendo un escalado automático. 
Los servicios clave seleccionados y su función en la arquitectura son:

\begin{itemize}
    \item \textbf{Amazon S3 (Simple Storage Service):} Actúa como la capa de persistencia 
    centralizada (Data Lake). Proporciona almacenamiento de objetos altamente escalable, 
    duradero y seguro, permitiendo separar los datos en diferentes zonas según su estado de 
    procesamiento (Raw vs Processed).
    \item \textbf{Amazon Kinesis Data Streams:} Facilita la ingesta de datos de alta 
    velocidad y baja latencia. Desacopla los productores de los consumidores, permitiendo 
    que múltiples aplicaciones procesen el flujo de datos simultáneamente.
    \item \textbf{Amazon Kinesis Data Firehose:} Gestiona la entrega fiable de los datos en 
    streaming hacia el almacenamiento en S3. Incluye capacidades de transformación en vuelo 
    (mediante AWS Lambda) y gestión de buffers para optimizar la escritura de ficheros.
    \item \textbf{AWS Glue:} Proporciona un entorno unificado para el descubrimiento de 
    metadatos (Data Catalog) y la ejecución de trabajos ETL (Extract, Transform, Load) 
    basados en Apache Spark, facilitando la limpieza y agregación de datos sin gestionar 
    servidores.
\end{itemize}

La arquitectura implementada procesa un conjunto de datos simulado referente a un inventario 
transaccional de dispositivos portátiles. Cada registro contiene atributos heterogéneos como 
marca, modelo, resolución de pantalla, especificaciones de hardware (CPU, RAM, GPU), 
sistema operativo y precio (\href{https://github.com/ivanperezdiaz829/Cloud-Computing-AWS-2/blob/main/datos.json}{\texttt{datos.json}}). 
El flujo termina con la generación de datasets analíticos en formato columnar (Parquet), listos 
para ser consultados mediante SQL en herramientas como Amazon Athena.
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\section{Desarrollo de las actividades}
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\subsection{Configuración del bucket S3}

La creación y configuración del bucket S3 se realiza de forma automatizada mediante el script
 de infraestructura como código (IaC) \hyperref[annex:deploy]{\texttt{deploy.ps1}}. El nombre 
 del bucket se genera de forma dinámica concatenando el prefijo \texttt{datalake-laptops-} 
 con el identificador único de la cuenta AWS (Account ID), asegurando así un espacio de 
 nombres globalmente único, requisito indispensable en S3.

\begin{lstlisting}[language=PowerShell, caption={Generación dinámica del nombre del bucket en deploy.ps1}]
$env:BUCKET_NAME = "datalake-laptops-$($env:ACCOUNT_ID)"
aws s3 mb s3://$env:BUCKET_NAME 2>$null
\end{lstlisting}

La estructura de carpetas definida sigue las de diseño de Data Lakes, organizando los 
datos por capas ("zonas") para facilitar la gobernanza y el ciclo de vida de la información:

\begin{itemize}
  \item \texttt{raw/}: Conocida como "Landing Zone". Almacena los datos en bruto tal cual 
  son recibidos desde Kinesis Firehose, en formato JSON. Esta capa es inmutable y sirve 
  como fuente de verdad histórica.
  \item \texttt{processed/}: Contiene los resultados refinados generados 
  por los trabajos ETL. Los datos aquí están limpios, enriquecidos y convertidos a formato 
  Parquet.
  \item \texttt{config/}, \texttt{scripts/}, \texttt{logs/}: Carpetas de soporte para scripts,
  configuraciones y auditoría.
\end{itemize}
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\subsection{Implementación del productor de datos}

El componente productor es una aplicación desarrollada en Python 
(\hyperref[annex:kinesis]{\texttt{kinesis.py}}) que simula la generación de eventos de 
negocio en tiempo real. Utilizando la librería \texttt{boto3}, el script lee un dataset base 
y envía registros individuales al servicio Amazon Kinesis Data Streams.

Un aspecto técnico crucial es el uso de la \textbf{Partition Key}. Cada registro enviado 
incluye una clave de partición basada en la marca del dispositivo (\texttt{Company}). 
Esto asegura que todos los datos de una misma marca (ej. "Apple" o "Dell") se dirijan 
siempre al mismo shard, manteniendo el orden secuencial.

\begin{lstlisting}[language=Python, caption={Envío de registros con Partition Key en kinesis.py}]
response = kinesis.put_record(
    StreamName=STREAM_NAME,
    Data=json.dumps(laptop),
    PartitionKey=str(company) # Agrupar shards por marca
)
\end{lstlisting}
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\subsection{Configuración del consumidor (Kinesis Firehose)}

Amazon Kinesis Data Firehose actúa como el puente entre el flujo de datos en tiempo real y 
el almacenamiento persistente. Antes de escribir en S3, Firehose invoca una función AWS 
Lambda (\hyperref[annex:firehose]{\texttt{firehose.py}}) que realiza tres tareas 
fundamentales:
\begin{enumerate}
    \item \textbf{Enriquecimiento temporal:} Añade un campo \texttt{processed\_at}.
    \item \textbf{Formato JSON Lines:} Convierte el objeto JSON estándar añadiendo un salto 
    de línea, requisito para que Athena procese múltiples objetos.
    \item \textbf{Particionamiento Dinámico:} Define la clave de partición basada en la fecha.
\end{enumerate}

\begin{lstlisting}[language=Python, caption={Transformación y formato JSON Lines en firehose.py}]
# Re-codificar para Firehose (Debe terminar en salto de linea)
output_payload = json.dumps(data_json) + '\n'
\end{lstlisting}
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\subsection{Configuración de AWS Glue}

AWS Glue se utiliza para el descubrimiento de metadatos y el procesamiento ETL. Primero, 
el script \hyperref[annex:deploy]{\texttt{deploy.ps1}} configura el crawler 
\texttt{laptops-raw-crawler} que infiere el esquema de los datos en bruto.

Posteriormente, se implementan dos trabajos (\textit{Glue Jobs}) basados en Apache Spark. 
Un desafío técnico resuelto fue la inconsistencia de tipos de datos en el campo de precio 
(\textit{Schema Evolution}), donde algunos valores eran enteros y otros decimales. 
Se utilizó \texttt{resolveChoice} para unificar el tipo a \texttt{double}.

\begin{lstlisting}[language=Python, caption={Resolución de ambigüedad de tipos en laptops\_analytics\_brand.py}]
# CORRECCION DE TIPO DE DATO (CHOICE)
# Esto fuerza a que Price_euros sea tratado siempre como double
try:
    dynamic_frame = dynamic_frame.resolveChoice(specs = [('Price_euros','cast:double')])
except:
    logger.warning("No se requirio resolveChoice o fallo...")
\end{lstlisting}
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\section{Diagrama del flujo de datos}

La arquitectura implementada establece un pipeline unidireccional que 
garantiza la integridad, trazabilidad y escalabilidad del dato. El flujo 
de la información atraviesa las siguientes etapas secuenciales:

\begin{enumerate}
    \item \textbf{Generación:} El script productor inyecta registros JSON 
    en el Stream de Kinesis simulando transacciones.
    \item \textbf{Transporte y Buffer:} Kinesis Data Streams recibe los 
    datos y Kinesis Firehose los consume, gestionando la memoria intermedia.
    \item \textbf{Transformación en Vuelo:} Firehose invoca a la función 
    Lambda para añadir metadatos temporales y formatear a JSON Lines.
    \item \textbf{Almacenamiento Raw:} Firehose vuelca los bloques de datos 
    en la zona \texttt{raw} de S3, particionados por fecha.
    \item \textbf{Catalogación:} El Crawler de Glue escanea S3 y actualiza 
    el esquema en el Catálogo de Datos.
    \item \textbf{Procesamiento Batch:} Los Glue Jobs leen del catálogo, 
    ejecutan transformaciones Spark en memoria distribuida y agregan la 
    información.
    \item \textbf{Almacenamiento Processed:} Los resultados finales se 
    escriben en la zona \texttt{processed} de S3 en formato Parquet.
    \item \textbf{Consumo:} Amazon Athena utiliza el catálogo actualizado 
    para lanzar consultas SQL sobre los datos procesados.
\end{enumerate}

La Figura 1 ilustra gráficamente la topología completa de la solución desplegada.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/arquitectura.jpeg} 
    \caption{Diagrama de arquitectura del Data Lake Serverless en AWS.}
    \label{fig:arquitectura}
\end{figure}
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\section{Presupuesto y estimación de costes}

La gestión de costes es fundamental en arquitectura Cloud. La estimación presentada a 
continuación se basa en la calculadora oficial de AWS (AWS Pricing Calculator) para la región 
\texttt{us-east-1}, considerando un escenario de operación mensual estándar con un flujo de 
datos moderado.

\begin{itemize}
    \item \textbf{Kinesis:} 1 Shard activo durante 730 horas/mes (24x7).
    \item \textbf{Glue:} Ejecución diaria de Crawlers y Jobs ETL (Worker Type G.1X), consumiendo recursos bajo demanda.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/price.jpg}
    \caption{Captura de la calculadora oficial de AWS con el desglose de costes estimado.}
    \label{fig:presupuesto}
\end{figure}
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\section{Conclusiones}

En esta práctica se han asentado conocimientos sobre el diseño y despliegue de una arquitectura para gestionar la incerción y procesamiento de datos dentro de la nube de AWS, adicionalmente, se ha aprendido a utilizar servicios como Kinesis, AWS Glue y S3.

Entre las principales lecciones aprendidas y dificultades superadas, destacan:
\begin{itemize}
    \item \textbf{Schema Evolution:} La importancia de manejar la heterogeneidad de los datos 
    numéricos (enteros vs floats) en Spark mediante \texttt{resolveChoice}, dado que el desconocimiento
    de las mismas ocasionaron muchos problemas y errores.
    \item \textbf{Infraestructura como Código:} El uso de scripts PowerShell 
    (\hyperref[annex:deploy]{\texttt{deploy}}, \hyperref[annex:limpieza]{\texttt{clean}} y \hyperref[annex:crawler]{\texttt{aws-crawler}}) 
    que ha permitido desplegar y destruir la infraestructura de forma repetible, evitando costes 
    fantasma y de manera automatizada.
    \item \textbf{Particionamiento:} Se ha comprobado cómo el particionamiento dinámico en la 
    ingesta mejora la organización del Data Lake.
\end{itemize}

%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\section{Referencias y bibliografía}

\begin{itemize}
    % Documentación Oficial
    \item Amazon Web Services. (2026). \textit{Amazon Kinesis Data Streams Developer Guide}. Recuperado de la documentación oficial de AWS.
    \item Amazon Web Services. (2026). \textit{AWS Glue Developer Guide}. Sección sobre ETL Jobs y DynamicFrames.
    \item Amazon Web Services. (2026). \textit{Amazon S3 User Guide}. Best practices for Data Lakes.
    \item Apache Spark. (2026). \textit{PySpark Documentation}. DataFrame API y tipos de datos.

    % Herramientas Web
    \item Amazon Web Services. (2026). \textit{AWS Pricing Calculator} [Web application]. https://calculator.aws/
    \item Lucid Software Inc. (2026). \textit{Lucidchart} [Web application]. https://www.lucidchart.com/

    % Inteligencia Artificial
    \item Google. (2026). \textit{Gemini} (Versión 1.5) [Large language model]. https://gemini.google.com/
    \item OpenAI. (2026). \textit{ChatGPT} (Versión GPT-4) [Large language model]. https://chat.openai.com/

    % Datos y Repositorios
    \item Mohammad Anas. (2023). \textit{Laptop Price Dataset} [Data set]. Kaggle. https://www.kaggle.com/datasets/anas123siddiqui/laptops
    \item Pérez, I. (2026). \textit{Cloud-Computing-AWS-2} [Software repository]. GitHub. https://github.com/ivanperezdiaz829/Cloud-Computing-AWS-2
\end{itemize}
%------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------
\section{Anexos}

El código fuente completo, así como el historial de versiones y la documentación adicional, se encuentra disponible en el siguiente repositorio público de GitHub:

\begin{center}
    \href{https://github.com/ivanperezdiaz829/Cloud-Computing-AWS-2}{\textbf{https://github.com/ivanperezdiaz829/Cloud-Computing-AWS-2}}
\end{center}

A continuación se incluye una copia estática de los scripts principales utilizados para la realización de la práctica.

\subsection{Anexo A. Runmap de Ejecución}
\label{annex:runmap}
Es una guía paso a paso de las ejecuciones necesarias para desplegar, probar y destruir la arquitectura.
\lstinputlisting[
  language=PowerShell,
  caption={Script runmap.ps1: Ejecución completa de la práctica}
]{./runmap.ps1}

\subsubsection*{Guía Visual de Ejecución}

\textbf{Paso 1: Despliegue de Infraestructura} \\
Ejecución de \texttt{deploy.ps1}. Se valida la creación del Bucket S3 que actuará como Data Lake.
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/bucket_S3.jpg}
    \caption{Creación del Bucket S3 y estructura de carpetas.}
\end{figure}

\textbf{Paso 2: Ingesta de Datos} \\
Monitorización del stream de Kinesis recibiendo los registros enviados por el productor Python.
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/stream.jpg}
    \caption{Métricas de ingesta en Kinesis Data Streams.}
\end{figure}

\textbf{Paso 3: Catalogación Raw} \\
Verificación de que los datos han aterrizado en la zona \texttt{raw/} de S3 tras pasar por Firehose y antes de ser catalogados.
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/post_Kinesis.jpg}
    \caption{Datos JSON aterrizados en S3 (Zona Raw).}
\end{figure}

\textbf{Paso 4: Procesamiento ETL} \\
Panel de AWS Glue mostrando la ejecución exitosa (\textit{Succeeded}) de los Jobs de Spark para Marcas y Sistemas Operativos.
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/ETL.jpg}
    \caption{Estado de ejecución de los Glue Jobs.}
\end{figure}

\textbf{Paso 5: Resultados Procesados} \\
Tras la ejecución del crawler de resultados, se generan las carpetas particionadas en formato Parquet en la zona \texttt{processed/}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/laptop_brand.jpg}
    \includegraphics[width=1.0\textwidth]{images/laptop_os.jpg}
    \caption{Estructura de salida en S3 para Marcas (up) y SO (down).}
\end{figure}

\textbf{Paso 6: Consultas SQL (Athena)} \\
Validación de los datos mediante consultas SQL estándar sobre las tablas catalogadas.

\textit{Consulta 1: Consulta al coste promedio a partir del SO.}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/SQL_1.jpg}
    \caption{Consulta SQL de verificación inicial.}
\end{figure}

\textit{Consulta 2: Análisis de precios por Marca.}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/SQL_2.jpg}
    \caption{Ranking de marcas por precio promedio.}
\end{figure}

\textit{Consulta 3: Análisis de la cantidad media de RAM.}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/SQL_3.jpg}
    \caption{Agregación de dispositivos por Sistema Operativo.}
\end{figure}

\subsection{Anexo B. Script de despliegue}
\label{annex:deploy}
Este script automatiza la creación de la infraestructura (Buckets, Roles, Streams, Lambdas y Jobs).
\lstinputlisting[
  language=PowerShell,
  caption={Script deploy.ps1: Automatización de infraestructura AWS}
]{./deploy.ps1}

\subsection{Anexo C. Productor Kinesis}
\label{annex:kinesis}
Script desarrollado en Python encargado de la simulación y envío de datos transaccionales al servicio Kinesis Data Streams.
\lstinputlisting[
  language=Python,
  caption={Script kinesis.py: Productor de datos a Kinesis}
]{./kinesis.py}

\subsection{Anexo D. Transformación Firehose}
\label{annex:firehose}
Función Lambda encargada de la transformación y particionado temporal de los datos antes de su persistencia en S3.
\lstinputlisting[
  language=Python,
  caption={Script firehose.py: Lógica de transformación Lambda}
]{./firehose.py}

\subsection{Anexo E. Análisis por Marca (Spark)}
\label{annex:brand}
Job de Spark ETL para la agregación de métricas de negocio basadas en el fabricante.
\lstinputlisting[
  language=Python,
  caption={Script laptops\_analytics\_brand.py: ETL de marcas}
]{./laptops_analytics_brand.py}

\subsection{Anexo F. Análisis por Sistema Operativo (Spark)}
\label{annex:os}
Job de Spark ETL para la agregación de métricas basadas en el Sistema Operativo.
\lstinputlisting[
  language=Python,
  caption={Script laptops\_analytics\_so.py: ETL de Sistemas Operativos}
]{./laptops_analytics_so.py}

\subsection{Anexo G. Crawler de Resultados}
\label{annex:crawler}
Script para catalogar los resultados finales procesados en formato Parquet.
\lstinputlisting[
  language=PowerShell,
  caption={Script aws\_crawler.ps1: Definición del Crawler}
]{./aws_crawler.ps1}

\subsection{Anexo H. Script de limpieza}
\label{annex:clean}
Script crítico para la eliminación de recursos y prevención de costes tras la finalización del laboratorio.
\lstinputlisting[
  language=PowerShell,
  caption={Script clean.ps1: Destrucción de recursos}
]{./clean.ps1}

\subsection{Anexo I. Dataset de Origen}
\label{annex:dataset}
Archivo JSON que contiene el inventario de dispositivos utilizado por el productor de datos.
\begin{center}
    \href{https://github.com/ivanperezdiaz829/Cloud-Computing-AWS-2/blob/main/datos.json}{\textbf{Ver archivo datos.json en GitHub}}
\end{center}

\subsection{Anexo J. Uso de la Inteligencia Artificial Generativa}

Se han utilizado principalmente 2 herramenientas de IA generativa para asistir en
la creación del formato del documento con el objetivo de que siga las restricciones impuestas,
así como para la revisión ortográfica y el ordenamiento lógico de los contenidos
de la memoria (siempre dentro del formato y estructura requerida).
\begin{itemize}
    \item \textbf{ChatGPT-4 (OpenAI):} Se ha empleado para generar la estructura inicial del documento con el formato básico.
    \item \textbf{Gemini (Google):} Se ha utilizado como asistente a lo largo de la realización del proyecto, para obtener definiciones de algunas de las tecnologías explicadas en el presente documento y para la resolución de errores como los surgidos por el problema de tipos en el precio de los ordenadores del dataset.
\end{itemize}
\end{document}